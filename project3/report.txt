The first task was to understand how bayesian optimization works. Following some online resources and the lecture materials, we first created the kernels as recommended in the task description and created two GaussianProcessRegressor with the kernels. Then we implemented the next_recommendation, where we fit the Regressors based on the sampled data and call the optimize acquisition function.
The other functions such as add_data_point and get_solution are straight-forward, i.e. in the first one we add the parameters to our arrays and in get_solution we first check for every datapoint that does not go over the safety-threshold and then get the parameter that achieves largest f-value and return that as our optimal solution.

The tricky part was the acquisition function. We used the upper confidence bound where the tradeoff-parameter is set to 1. However, we punish the acquisition value, if the predicted mean of v plus standard-deviation of v times a regularizer term is larger than the safety threshold i.e. we substract mu_f by the amount that is above the threshold multiplied by another regularizer (we doubled it). In other words, candidates that have a chance of being an unsafe evalutation get a lower mu_f, meaning they will be unlikely to be chosen.

We also tried out a DotProduct-Kernel with sigma=1 which achieved slightly better results than a basic linear-kernel (sigma=0). Therefore, we chose that one.